---
title: "UW Datathon 2026 Preprocessing"
author: "Kirby Vo"
date: "2026-02-07"
output: pdf_document
---

```{r}
library(tidyverse)
library(stringr)
```

# LOAD DATA

```{r}
data <- read_delim("datasets/Access_to_Tech_Dataset.csv")
```

# VERIFY DATA

```{r}
data |>
  names()

data |>
  sample_n(10)
```
# CHECK NEED TO CLEAN DATA

Based on the scrape status, we believe it is safe to assume there isn't any incomplete data to filter out.

```{r}
data |>
  distinct(scrape_status)

```

Looking at the distinct domain categories, we see that there are inconsistentcies in data entries. For example there are entries for either Ecommerce and E-commerce which would mess up our data aggregation.

```{r}
data |>
  distinct(domain_category)
```

Therefore we will set up a map so that we can rename the values to their correct name.

```{r}
renaming_map <- c(
  "Government and Public Services" = "Government and Public Services",
  "News and Media" = "News and Media",
  "Technology Science and Research" = "Technology Science and Research",
  "E-commerce" = "E-commerce",
  "Educational Platforms" = "Educational Platforms",
  "Streaming Platforms" = "Streaming Platforms",
  "Health and Wellness" = "Health and Wellness",
  "TechnologyScienceResearch" = "Technology Science and Research",
  "Ecommerce" = "E-commerce"
)

data_clean <- data |>
  mutate(
    domain_category = renaming_map[domain_category]
  )


```

Verify the cleaned data.

```{r}
data_clean |>
  distinct(domain_category)
```

# INVESTIGATE A SPECIFIC WEBPAGE
Looking at one specific web url, we can see that there are multiple row for the same webpage, with different violations recorded for each data entry.

```{r}
data_clean |>
  filter(web_URL_id == 700)
  
```

# GROUPING DATA BY WEBPAGE
Here we will group our data by web url for aggregation and calculating our metrics.

```{r}
grouped_data <- data_clean |>
  group_by(web_URL_id, web_URL, domain_category) |>
  summarize(
    violations = list(
      tibble(
          name = violation_name,
          score = violation_score,
          category = violation_category,
          impact = violation_impact,
          wcag_reference = wcag_reference
      )
    )
  )
  
```

# WEBPAGE LEVEL METRICS

```{r}
data_summary <- grouped_data %>%
  mutate(
    n_violations = map_int(violations, nrow),
    avg_severity = map_dbl(violations, ~ mean(.x$score, na.rm = TRUE)),
    max_severity = map_int(violations, ~ max(.x$score, na.rm = TRUE)),
    min_severity = map_int(violations, ~ min(.x$score, na.rm = TRUE)),
    avg_impact = map_dbl(violations, ~ min(.x$impact, na.rm = TRUE)),
    n_violation_types = map_int(violations, ~ n_distinct(.x$name)),
    n_violation_categories = map_int(violations, ~ n_distinct(.x$category))
  )
```

```{r}
data_summary |>
  head(5)
```


# DOMAIN LEVEL METRICS


```{r}
domain_summary <- data_summary |>
  group_by(domain_category) |>
  summarize(
    n_pages = n(),
    total_violations = sum(n_violations),
    avg_violations_per_page = mean(n_violations),
    avg_severity = mean(avg_severity)
  )
```

```{r}
domain_summary
```

```{r}
barplot(table(data$violation_name))
```



