---
title: "UW Datathon 2026 Preprocessing"
author: "Kirby Vo, Kelbin Luong"
date: "2026-02-07"
output: pdf_document
---

```{r}
library(tidyverse)
library(stringr)
library(writexl)
```

# LOAD DATA

```{r}
data <- read_delim("datasets/Access_to_Tech_Dataset.csv")
```

# PREVIEW DATA

```{r}
data |>
  names()

data |>
  sample_n(10)
```
# SEARCH FOR DATA INCONSISTENCIES

We found that looking at distinct categories, we saw some inconsistent domain category entries that we would need to remap

```{r}
data |>
  distinct(domain_category)
```
We also found cases where if a url had multiple domain categories assigned to it, any violation entries for it would be duplicated accordingly.

```{r}
data |>
  filter(web_URL == "https://arstechnica.com/health/") |>
  select(domain_category, web_URL, violation_name, violation_score) |>
  head(2)
```


We had also noticed while processing the data that there were instances of urls with or without /, which would count them as seperate webpages despite being the same, which needed to be addressed.

```{r}
data |> 
  filter(web_URL == "https://www.nbcnews.com" | web_URL == "https://www.nbcnews.com/") |>
  select(web_URL) |>
  slice(1, 22)

```
Regarding domains, we found that there were a lot of domains such as arstechnica.com which had multiple webpages under it, which meant we would need to group them properly by domain in order to answer domain specific questions.

```{r}
data |> 
  filter(str_detect(web_URL, "arstechnica\\.com")) |>
  distinct(web_URL) |>
  head(5)

```


#  CLEAN DATA

Here we create a map to convert weird domain category values to correct ones.

```{r}
renaming_map <- c(
  "Government and Public Services" = "Government and Public Services",
  "News and Media" = "News and Media",
  "Technology Science and Research" = "Technology Science and Research",
  "E-commerce" = "E-commerce",
  "Educational Platforms" = "Educational Platforms",
  "Streaming Platforms" = "Streaming Platforms",
  "Health and Wellness" = "Health and Wellness",
  "TechnologyScienceResearch" = "Technology Science and Research",
  "Ecommerce" = "E-commerce"
)

data_clean <- data |>
  mutate(
    domain_category = renaming_map[domain_category],
    domain = web_URL |>
      str_replace("^https?://(www\\.)?", "") |>
      str_replace("/.*$", "")
  )

```



Ensure that violations aren't duplicated based off numebr of additional domain categories.

```{r}
data_clean_deduped <- data_clean |>
  distinct(web_URL,
           violation_name,
           violation_score,
           violation_category,
           violation_impact,
           wcag_reference,
           .keep_all = TRUE)

```

## VERIFY PROBLEMS FIXXED

```{r}
data_clean_deduped |>
  distinct(domain_category)
```

```{r}
data_clean_deduped |> 
  filter(web_URL == "https://www.nbcnews.com/" | web_URL == "https://www.nbcnews.com") |>
  select(web_URL, domain) |>
  slice(c(1, 22))
```

```{r}
data_clean_deduped |>
  filter(web_URL == "https://arstechnica.com/health/") |>
  select(domain_category, web_URL, violation_name, violation_score) |>
  head(2)
```

# AGGREGATE DATA

Here we aggregated data per page for potentially useful metrics to include in individual summaries we will input into tableau and graph. (We didn't end up using the aggregated values calculated here.)

```{r}
page_summary <- data_clean_deduped |>
  group_by(web_URL, domain) |>
  summarise(
    n_violations = n(),
    avg_severity = mean(violation_score, na.rm = TRUE),
    max_severity = max(violation_score, na.rm = TRUE),
    min_severity = min(violation_score, na.rm = TRUE),
    n_distinct_violation_types = n_distinct(violation_name),
    n_distinct_violation_categories = n_distinct(violation_category),
    .groups = "drop"
  )

```


# DOMAIN-LEVEL SUMMARY

```{r}
domain_summary <- page_summary |>
  group_by(domain) |>
  summarise(
    n_pages = n(),
    total_violations = sum(n_violations, na.rm = TRUE),
    avg_violations_per_page = mean(n_violations, na.rm = TRUE),
    avg_severity = mean(avg_severity, na.rm = TRUE),
    .groups = "drop"
  )


```

```{r}
domain_summary |>
  head(4)
```


```{r}
domain_category_summary <- data_clean |>
  group_by(domain_category) |>
  summarise(
    total_violations = n(),
    avg_severity = mean(violation_score, na.rm = TRUE),
    .groups = "drop"
  )

```

```{r}
domain_category_summary |>
  head(4)
```


# VIOLATION-LEVEL SUMMARIES

```{r}
violation_summary <- data_clean |>
  count(violation_name, sort = TRUE) |>
  rename(violations = n)

violation_type_by_domain_category <- data_clean |>
  group_by(domain_category, violation_name) |>
  summarise(violations = n(),
            .groups = "drop")

violation_category_by_domain_category <- data_clean |>
  filter(!is.na(violation_category)) |>
  group_by(domain_category, violation_category) |>
  summarise(violations = n(),
            .groups = "drop")


violation_category_summary <- data_clean |>
  filter(!is.na(violation_category)) |>
  count(violation_category, sort = TRUE) |>
  rename(violations = n)


top_violations <- violation_summary |>
  slice_max(violations, n = 10) |>
  pull(violation_name)

violation_summary_grouped <- violation_summary |>
  mutate(
    violation_group = if_else(
      violation_name %in% top_violations,
      violation_name,
      "Other"
    )
  ) |>
  group_by(violation_group) |>
  summarise(violations = sum(violations), .groups = "drop")
```

```{r}
violation_summary  |>
  head(4)
```

```{r}
violation_type_by_domain_category  |>
  head(4)
```

```{r}
violation_category_by_domain_category  |>
  head(4)
```

```{r}
violation_category_summary|>
  head(4)
```

```{r}
violation_summary_grouped  |>
  head(4)
```


# EXPORT EVERYTHING FOR TABLEAU

```{r}
write.csv(domain_summary, "processed_data/domain_summary.csv", row.names = FALSE)
write.csv(domain_category_summary, "processed_data/domain_category_summary.csv", row.names = FALSE)
write.csv(violation_summary, "processed_data/violation_summary.csv", row.names = FALSE)
write.csv(violation_summary_grouped, "processed_data/violation_summary_grouped.csv", row.names = FALSE)
write.csv(violation_category_summary, "processed_data/violation_category_summary.csv", row.names = FALSE)
write.csv(violation_type_by_domain_category, "processed_data/violation_type_by_domain.csv", row.names = FALSE)
write.csv(violation_category_by_domain_category, "processed_data/violation_category_by_domain.csv", row.names = FALSE)
```

finished :D


